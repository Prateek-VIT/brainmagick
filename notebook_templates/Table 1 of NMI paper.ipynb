{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hostname SCOPE-208-1 not defined in /conf/study_paths/study_paths.yaml. Using default paths.\n",
      "2024-02-12 12:39:20.623065: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-12 12:39:20.652928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-12 12:39:21.147517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import mne\n",
    "import torch\n",
    "import numpy as np\n",
    "import bm\n",
    "from bm import play\n",
    "from bm.train import main\n",
    "from bm.events import Word\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display as disp\n",
    "\n",
    "mne.set_log_level(False)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "os.chdir(main.dora.dir.parent)\n",
    "os.environ['NO_DOWNLOAD'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigs = ['34219380', '6e3bf7d7', '557f5f8a', '4395629c']\n",
    "sigs = ['b3823d04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:bm.play:Loading solver from XP b3823d04. Overrides used: ['norm.max_scale=20', 'dset.n_recordings=4', 'model=clip_conv', 'optim.batch_size=16', 'dset.selections=[\"brennan2019\"]', 'optim.loss=\"mse\"', 'dset.features=[\"MelSpectrum\"]']\n",
      "WARNING:bm._env:Hostname SCOPE-208-1 not defined in /conf/study_paths/study_paths.yaml. Using default paths.\n",
      "INFO:bm.dataset:Loading Subjects | 1/4 | 2.94 it/sec\n",
      "INFO:bm.dataset:Loading Subjects | 2/4 | 4.03 it/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:bm.dataset:Loading Subjects | 3/4 | 4.99 it/sec\n",
      "INFO:bm.dataset:# Examples (train | valid | test): 3184 | 748 | 758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prateek/anaconda3/envs/bm/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (120) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:bm.train:Model hash: d8e8c7bfefa82e53bcee0d8974eb27f941edac13\n",
      "ALL SOLVERS LOADED\n",
      "now the table.\n",
      "brennan2019 60 & 4 & inf h& 1065 & 513& 190 & 148\\\\\n",
      "Vocab overlap: 59.5%\n"
     ]
    }
   ],
   "source": [
    "def _get_segments_and_vocabs(solver):\n",
    "    from scripts.run_eval_probs import _get_extra_info\n",
    "    per_split = {}\n",
    "    for split in ['train', 'test']:\n",
    "        segments = set()\n",
    "        sentences = set()\n",
    "        vocab = set()\n",
    "        dset = getattr(solver.datasets, split)\n",
    "        loader = solver.make_loader(dset, shuffle=False)\n",
    "        for idx, batch in enumerate(loader):\n",
    "            data, *_ = _get_extra_info(batch, solver.args.dset.sample_rate)\n",
    "            time_to_main_word = 0 - solver.args.dset.tmin  # location of main word relative to segment start\n",
    "            # e.g. with MNE we have tmin=-0.5 so the main word is 0.5 seconds after start of MNE Epoch.\n",
    "            margin = 2 # we need to look a bit after 0.5 due to rounding error, this is in time steps.\n",
    "            look_at_index = int(time_to_main_word * solver.args.dset.sample_rate + margin)\n",
    "            word_index = data[:, 0, look_at_index]\n",
    "            sequence_id = data[:, 1, look_at_index]\n",
    "            segment_ids = list(zip(word_index.tolist(), sequence_id.tolist()))\n",
    "            \n",
    "            segment_duration = data.shape[-1] / solver.args.dset.sample_rate\n",
    "            for events in batch._event_lists:\n",
    "                for event in events:\n",
    "                    if isinstance(event, Word):\n",
    "                        start = event.start - events[0].start\n",
    "                        end = start + event.duration\n",
    "                        if end > 0.02 and start < segment_duration - 0.02:\n",
    "                            # due to rounding errors, retrieval of related events\n",
    "                            # can sometime overlap in a non meaningful way, e.g. less than 20ms.\n",
    "                            # we only consider an event if it overlaps for at least 20ms.\n",
    "                            sentences.add(event.word_sequence)\n",
    "                            vocab.add(event.word)\n",
    "            segments |= set(segment_ids)\n",
    "#             print(idx, len(loader), end='\\r')\n",
    "#         print(split, \"done\", \" \" * 400)\n",
    "        per_split[split] = (segments, vocab, sentences)\n",
    "    return per_split\n",
    "\n",
    "\n",
    "def print_table_line(solver):\n",
    "    channels = solver.datasets.train[0].meg.shape[0]\n",
    "    n_subjects = len(set([dataset.recording.subject_uid for dataset in solver.datasets.train.datasets]))\n",
    "    per_split = _get_segments_and_vocabs(solver)\n",
    "    assert len(solver.args.dset.selections) == 1\n",
    "    name = solver.args.dset.selections[0]\n",
    "    duration = 0.\n",
    "    for dset in solver.datasets.train.datasets:\n",
    "        events = dset.recording.events()\n",
    "        duration += (events.start + events.duration).max()\n",
    "    \n",
    "    print(name, channels, '&' , n_subjects, '&', format(duration/ 3600, '.1f') + ' h', end='')\n",
    "    for split in ('train', 'test'):\n",
    "        segments, vocab, sentences = per_split[split]\n",
    "        print('&', len(segments), '&', len(vocab), end='')\n",
    "    vocab_train = per_split['train'][1]\n",
    "    vocab_test = per_split['test'][1]\n",
    "    vocab_overlap = len(vocab_train & vocab_test) / len(vocab_test)\n",
    "#     print('&', format(vocab_overlap, '.1%'), end='')\n",
    "    print(r'\\\\')\n",
    "    print(\"Vocab overlap:\", format(vocab_overlap, '.1%'))\n",
    "    \n",
    "solvers = [play.get_solver_from_sig(sig) for sig in sigs]\n",
    "print(\"ALL SOLVERS LOADED\")\n",
    "print(\"now the table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brennan2019 60 & 4 & inf h& 1065 & 513& 190 & 148\\\\\n",
      "Vocab overlap: 59.5%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for solver in solvers:\n",
    "    print_table_line(solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_map(solver):\n",
    "    loader = solver.make_loader(solver.datasets.train)\n",
    "    batch = next(iter(loader)).to(solver.device)\n",
    "    model = solver.model\n",
    "    merger = model.merger\n",
    "    positions = merger.position_getter.get_positions(batch)\n",
    "    embedding = merger.embedding(positions)\n",
    "    meg = batch.meg\n",
    "    B, C, T = meg.shape\n",
    "    score_offset = torch.zeros(B, C, device=meg.device)\n",
    "    score_offset[merger.position_getter.is_invalid(positions)] = float('-inf')\n",
    "    heads = merger.heads[None].expand(B, -1, -1)\n",
    "    scores = torch.einsum(\"bcd,bod->boc\", embedding, heads)\n",
    "    scores += score_offset[:, None]\n",
    "    weights = torch.softmax(scores, dim=2)\n",
    "    \n",
    "    # Weights is of shape [Virtual Channels, Input Channels]\n",
    "    # Each Virtual Channel is a weighted sum over the input channels.\n",
    "    # Positions give the normalized 2d position for each Input channel.\n",
    "    # To get an overall weight for a given input sensor you can for instance do\n",
    "    # weights[0].sum(dim=0)\n",
    "    return weights[0], positions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mweights\u001b[49m\u001b[38;5;241m.\u001b[39mshape, positions\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "weights.shape, positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
